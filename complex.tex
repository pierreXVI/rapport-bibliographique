\chapter{Méthodes plus complexes}

\paragraph{}
Dans les chapitres précédents, nous avons présentés des outils extraits de la littérature permettant de résoudre des équations différentielles.
Nous avons expliqué comment, en partant d'une équation aux dérivées partielles issue du modèle physique, nous arrivions après une étape de discrétisation numérique à une équation différentielle ordinaire.
Des méthodes d'intégration temporelle ont été présentée afin de résoudre cette équation, et nous avons mis en avant une classe d'intégrateurs : les méthode implicites.
Ces méthodes permettent de calculer l'état du système en résolvant des équations non-linéaires, qui sont elles même résolue avec la solution de systèmes linéaires.
Enfin, nous avons identifié une méthode de Krylov, GMRES, et certaines de ses améliorations, pour pouvoir résoudre un système linéaire.

\paragraph{}
Ce cheminement permet l'intégration temporelle de phénomènes physiques, mais à nouveau ce n'est pas le seul possible.
On peut envisager d'utiliser des méthodes et des techniques moins classiques pour arriver à nos fins.


\section{Pour la résolution du système linéaire}


  \subsection{Formulation sans matrice}

    \paragraph{}
    Dans le cadre de la simulation numérique de grande dimension, une formulation apparaît comme très séduisante.
    Il s'agit des méthodes Jacobian Free Newton Krylov (JFNK), utilisées pour résoudre le système linéaire.
    Ces méthodes sont dites "sans matrices", car elles ne nécessitent pas la formation explicite de la matrice du système linéaire.
    Cela n'est pas anodin, car puisqu'on travaille sur des maillages de très grande taille, la matrice est donc de très grande dimension.
    La calculer a alors un certain coût algorithmique, et la stocker a un grand coût en mémoire.

    \subsubsection{Calcul de la jacobienne}

      \paragraph{}
      L'équation (\ref{eq:linear}) à laquelle nous étions arrivé cherchait à résoudre un système linéaire où la matrice à inverser était de la forme :
      \[A = \operatorname{Id} - \Delta t\frac{\partial F}{\partial W}\left(W_0\right)\]
      pour une certaine fonction $F$ que nous avions identifiée.
      Former la jacobienne de $F$ est un problème à part entière.
      En pratique, on réalise l'un des choix suivants.

      \paragraph{}
      On peut former analytiquement la jacobienne à partir de l'expression de $F$.
      Cela semble donc être le meilleur choix, car on aurait la valeur exacte de l'opérateur pour une complexité algorithmique moindre.
      Cependant, cela nécessite un travail important des développeurs.
      En pratique, un code de calcul a pour vocation d'évoluer en intégrant de nouveaux modèles physiques, et pour cette raison cette solution n'est pas toujours utilisée.
      En effet, une modification ou un ajout d'un modèle entraînent une modification de la jacobienne, et le code doit être scrupuleusement adapté.
      Le lourd travail de calcul analytique de la jacobienne, et la pénibilité de la gestion du code font que ce choix n'est pas le seul utilisé.
      Certains codes de calcul se contentent eux d'une approximation analytique très grossière de la jacobienne.
      Si cela permet de la former de manière très économe, nous verrons par la suite quels sont les inconvénients de ce choix.

      \paragraph{}
      On peut calculer la jacobienne à partir de $F$, en utilisant des différences finies.
      En effet, la $i$\textsuperscript{ème} colonne de la jacobienne de $F$ peut être approchée par :
      \[\frac{\partial F}{\partial W}\left(W_0\right)_i = \frac{F\left(W_0 + \varepsilon e_i\right) - F\left(W_0\right)}{\varepsilon} + O\left(\varepsilon\right)\]
      pour un $\varepsilon$ suffisamment petit, avec $e_i$ le $i$\textsuperscript{ème} vecteur de la base canonique.
      On pourrait atteindre une meilleure précision avec des différences finies centrées, cependant on dispose déjà, en général, de la valeur de $F\left(W_0\right)$, et les différences finies décentrées permettent donc d'économiser une évaluation de $F$.
      Le choix du paramètre $\varepsilon$ doit être fait avec beaucoup de vigilance.
      Une valeur trop grande donnerait une mauvaise approximation de la jacobienne, alors qu'une valeur trop faible entraînerait des erreurs d'arrondis numériques.
      On pourrait envisager d'utiliser la méthode du "Complex-Step Differentiation" \footnote{\PS{J'ai une ref mais ça doit être à l'ONERA, donc TODO}} dans un code de calcul travaillant sur des nombres complexes, mais ce n'est pas notre cas a priori.

      \paragraph{}
      Calculer la jacobienne avec des différences finies donnerait une approximation à l'ordre 1 pour un coût de calcul important : cela nécessite $N$ évaluations de la fonction $F$.
      Pour éviter ces opérations coûteuses, on peut utiliser un coloriage de la jacobienne \cite{GebremedhinMannePothen2005}.
      Cela consiste à identifier des colonnes "indépendantes", auxquelles on associe une couleur, qu'on va calculer par une unique différence finie.
      On peut calculer un coloration de la jacobienne car on peut en général obtenir son pattern de creusité facilement puisqu'un connaît le schéma de discrétisation du code.

    \subsubsection{JFNK}

      \paragraph{}
      Plutôt que de calculer entièrement la jacobienne, chose coûteuse en temps et en mémoire comme on l'a dit précédemment, on peut se contenter d'approcher son action.
      En effet, lorsque nous avions introduit les méthodes de Krylov, nous avions remarqué qu'il n'était pas nécessaire de connaître explicitement $A$, mais seulement de pouvoir calculer le produit $Av$ pour un vecteur $v$ donné.
      L'idée des méthodes JFNK est donc de prendre, pour $v$ donné,
      \[Av = \left(\operatorname{Id} - \Delta t\frac{\partial F}{\partial W}\left(W_0\right)\right)v \approx v - \Delta t\frac{F\left(W_0 + \varepsilon v\right) - F\left(W_0\right)}{\varepsilon}\ .\]

      \paragraph{}
      Là aussi, le paramètre $\varepsilon$ doit être consciencieusement choisi.
      Des études nous guident vers certains choix pour la valeur de ce paramètre \cite{KnollKeyes2004}.
      Un avantage de cette approximation de la jacobienne apparaît pour les calculs multiphysiques.
      Pour ces applications, les physiques ont souvent des échelles de variations différentes, donc on ne peut pas prendre $\varepsilon$ adapté à toutes les physiques.
      Ce n'est pas un problème, au contraire, puisqu'on prendra un $\varepsilon$ par physique \cite{Turpault2003}.

      \paragraph{}
      Ces algorithmes JFNK ont démontré leur utilité sur des applications concrètes \cite{LiuZhangZhongEtAl2015, FrancoCamierAndrejEtAl2020}.


  \subsection{Différenciation Automatique}

    \paragraph{}
    Si les méthodes JFNK offrent de bonnes performances, elles ont un défaut qui est l'erreur induite sur l'approximation de la jacobienne.
    Si la jacobienne est mal approximé, alors le système linéaire résolu n'est pas le bon, et donc la résolution de l'équation non-linéaire sera gênée.
    Une solution applicable est l'utilisation de la Différenciation Automatique (AD) \cite{Griewank2000}.
    Cela consiste à "dériver" à l'aide d'un programme externe \cite{HascoeetPascual2012} le code source de notre solveur.

    \paragraph{}
    Un avantage de la différenciation automatique est que son coût n'est payé qu'à la compilation du code, au moment de sa différenciation.
    Après cela, le calcul de la jacobienne se fait par le simple appel à une fonction, qui a été automatiquement produite par l'outil.
    Un autre avantage est que le résultat obtenu est exact, et non approché.
    La précision obtenue est alors bien meilleure qu'avec des différences finies.

    \paragraph{}
    Une idée est alors d'utiliser les avantages de JFNK avec la précision de la différenciation automatique.
    En effet, si l'AD peut former explicitement la jacobienne, elle peut plus rapidement encore calculer son action sur un vecteur.
    On va donc alors utiliser une méthode de Krylov, par exemple GMRES, sans former explicitement la matrice à inverser mais en calculant seulement son action sur un vecteur avec l'AD.

    \paragraph{}
    Si la différenciation automatique n'est pas très largement utilisée par la communauté de CFD, elle séduit de plus en plus \cite{BilanceriBeuxElmahiEtAl2011, KenwayMaderHeEtAl2019}.
    \footnote{\PS{Parler d'ELSA qui s'y met, ou alors on les aime pas donc on en parle pas ? Si oui je pense pouvoir trouver un doc où ils comparent les perfo avec/sans AD.}}


\section{Pour l'intégration temporelle}

  \subsection{Méthodes IMEX}

    \paragraph{}
    Dans la partie sur les méthodes d’intégration temporelle nous avons mis en évidence l'opposition entre les méthodes explicites et les méthodes implicites.
    Nous avons alors expliqué que pour notre cadre d'application il était nécessaire d'utiliser des méthodes implicites.
    La raison venait du fait que nous souhaitons utiliser des grands pas de temps, et que les méthodes explicites seront instables dans ce cas.

    \paragraph{}
    L’inconvénient des méthodes implicites est leur lourd coût de calcul.
    Une idée permet alors d'alléger ce coût.
    Considérons par exemple que le second membre de l'équation différentielle (\ref{eq:edo}) puisse se décomposer en une partie non raide $f$ et une partie raide $g$ :
    \[\frac{\partial W}{\partial t} = f\left(W\right) + g\left(W\right)\ .\]
    On peut envisager dans notre cas de prendre pour l'opérateur raide le flux associé à la turbulence visqueuse dans la couche limite, ou encore associé à la chimie réactive.
    L'idée est alors d'intégrer $g$ implicitement, car la raideur nous l'oblige, et d'intégrer $f$ explicitement, car on peut le faire et que c'est plus économe.
    Autrement dit, pour reprendre les termes de l'analyse de stabilité des méthodes, on intègre avec une méthode explicite la partie de $F$ correspondant aux valeurs propres dans le domaine de stabilité, et on intègre les autres valeurs propres avec une méthode implicite.
    Cette méthodologie est nommée IMEX, pour IMplicite-EXplicite.

    \paragraph{paragraph name}
    On peut par exemple utiliser une méthode de Runge-Kutta explicite, couplée à une méthode de Runge-Kutta implicite, pour obtenir une méthode ASIRK (Additive Semi-Implicit Runge-Kutta) \cite{Zhong1996}.
    Le schéma devient alors une somme entre le schéma explicite (\ref{eq:rk}) et l'implicite (\ref{eq:rk_impl}) :
    \[\left\{\begin{aligned}
      W_{n+1} &= W_n + \Delta t\sum_{i = 1}^k\hat{b}_if\left(W_{n,i}\right) + \Delta t\sum_{i = 1}^kb_ig\left(W_{n,i}\right)\\
      \;\textrm{avec}\quad W_{n,i} &= W_n + \Delta t\sum_{j = 1}^{i-1}\hat{a}_{ij}f\left(W_{n,j}\right) + \Delta t\sum_{j = 1}^{i}a_{ij}g\left(W_{n,j}\right) \\
    \end{aligned}\right.\ .\]

    \paragraph{}
    On peut ensuite développer tout un ensemble de variantes, en utilisant pour le schéma implicite une méthode DIRK, ou encore de Rosenbrock.
    Si pour ce genre de méthodes, les notions d'ordre et de stabilité doivent être approfondies, on peut développer des méthodes A-stable d'ordre arbitrairement élevé.
    Cependant, comme pour les méthodes de Runge-Kutta implicites, plus l'ordre de la méthode augmente et moins celle ci est réalisable sur les ordinateurs d'aujourd'hui.

    \paragraph{}
    Aujourd'hui, ces méthodes ASIRK sont encore d'actualité, et on trouve des utilisations sur des cas concrets comme des calculs d'interactions fluide structure \cite{HuangPerssonZahr2019}.


  \subsection{Méthode de Newton pour la multiphysique}

    \paragraph{}
    En suivant la même idée que pour les méthodes IMEX, on peut séparer l'opérateur autrement.
    On cherche ici à obtenir la solution de l'équation stationnaire, soit
    \[0 = F\left(W\right)\ .\]
    On considère maintenant que le second membre $F$ correspond à un problème multiphysique et peut être décomposée sous la forme :
    \[F\left(W\right) = \left(F_1\left(W\right), F_2\left(W\right)\right) \quad\textrm{avec}\quad W = \left(W_1, W_2\right)\ .\]

    \paragraph{}
    Une méthode classique pour résoudre l'équation stationnaire est d'utiliser une méthode de Newton sur $F$.
    Cependant, comme mentionné lors de l'introduction, l'algorithme aura du mal à converger à cause de la forte non-linéarité de $F$.

    \paragraph{}
    Une autre idée est alors d'utiliser une méthode du point fixe :
    À l'itération $k$, en connaissant $W_k = \left(W_{1,k}, W_{2,k}\right)$, on résout :
    \[\left\{\begin{aligned}
      F_1\left(W_{1*}, W_{2*}\right) &= 0 \\
      b  \left(W_{1*}, W_{2*}\right) &= b\left(W_{1,k}, W_{1,k}\right)
    \end{aligned}\right.\]
    puis :
    \[\left\{\begin{aligned}
      F_2\left(W_{1,k+1}, W_{2,k+1}\right) &= 0 \\
      c  \left(W_{1,k+1}, W_{2,k+1}\right) &= c\left(W_{1*}, W_{1*}\right)
    \end{aligned}\right.\]
    où $b$ et $c$ sont des contraintes sur la résolution, par exemple $b\left(x_1, x_2\right) = x_2$ et $c\left(x_1, x_2\right) = x_1$.
    Cette méthode est désignée méthode SIFP (Sequential-Implicit Fixed Point), car on calcule séquentiellement des points fixes des différentes physiques.

    \paragraph{}
    On note $\transpose{\left(W_{1**}\left(W_k\right), W_{2**}\left(W_k\right)\right)}$ la valeur de l'état donnée par une étape de la méthode SIFP en partant de $W_k$.
    Une troisième idée appelée méthode SIN (Sequantial-Implicit Newton) \cite{WongKwokHorneEtAl2019}, est d'appliquer une méthode de Newton pour résoudre :
    \[G\left(W\right) = W - \begin{pmatrix}W_{1**}\left(W\right) \\ W_{2**}\left(W\right)\end{pmatrix} = 0\ .\]
    L'origine de cette idée est que lorsque la méthode SIFP converge, alors l'équation précédente est vérifiée.
    On résout donc à chaque itération :
    \[\frac{\partial G}{\partial W}\left(W_k\right)\left(W_{k+1} - W_k\right) = -G\left(W_k\right)\ .\]

    \paragraph{}
    Dans \cite{WongKwokHorneEtAl2019}, l'idée est d'appliquer une méthode de Newton à $G$, et de résoudre les systèmes linéaires successifs avec GMRES.
    Comme nous l'avons proposés précédemment dans ce rapport, ils utilisent le fait que GMRES n'a pas besoin de former explicitement la matrice à inverser, et utilisent la différenciation automatique pour obtenir l'effet de la jacobienne de $G$ sur un vecteur.
    L'efficacité de la méthode SIN par rapport à la méthode SIFP à été démontrée sur un cas de couplage entre la mécanique des fluides et la thermique, et un cas de couplage entre la mécanique des fluides et la mécanique.


  \subsection{Méthodes exponentielles}

    \paragraph{}
    Les méthodes exponentielles apparaissent de plus en plus comme une alternative aux méthodes d'intégration classiques.
    Ces méthodes possèdent elles aussi cette dualité explicite-implicite qu'ont les méthodes classiques, mais leur principe reste commun.

    \paragraph{}
    On considère toujours l'équation différentielle ordinaire (\ref{eq:edo}) qu'on cherche à intégrer entre $t_n$ et $t_{n+1}$ pour trouver la valeur de l'état $W_{n+1}$.
    On décompose alors le second membre pour obtenir :
    \[\frac{\mathrm{d}W}{\mathrm{d}t} = AW + f\left(W\right)\ ,\]
    en prenant par exemple $A = \frac{\mathrm{d}F}{\mathrm{d}W}$ et $f\left(W\right) = F\left(W\right) - AW$ ou toute autre décomposition de cette forme.
    On peut alors intégrer de manière exacte entre $t_n$ et $t_{n+1}$ pour obtenir :
    \[W_{n+1} = e^{\Delta tA}W_n + \int_{t_n}^{t_{n+1}}e^{\left(t_{n+1}-t\right)A}f\left(W\left(t\right)\right)\mathrm{d}t\ .\]
    On constate alors que la partie linéaire de l'équation à été intégrée exactement, et que la partie non-linéaire doit encore être approchée.

    \paragraph{}
    La manière dont l'intégrale de la partie non-linéaire est calculée dépend de la méthode choisie.
    On peut par exemple calculer cette intégrale par une méthode de Taylor \cite{KoskelaOstermann2013}, ou bien avec une méthode de Runge-Kutta ou de Rosenbrock \cite{HochbruckOstermann2010}.

    \paragraph{}
    Le grand intérêt de ces méthode est l'intégration exacte de la partie linéaire.
    Cependant, cela se fait en échange du calcul de l'exponentielle d'une matrice.
    C'est une opération coûteuse numériquement, mais il est possible d'utiliser des méthodes efficaces.

    \paragraph{}
    La décomposition de l'opérateur sur des espaces de Krylov permet de calculer l'effet de l'exponentielle d'une matrice contre un vecteur.
    En effet, pour un vecteur donné $v$, en prenant la relation d'Arnoldi (\ref{eq:arnoldi}) après avoir construit la base de l'espace de Krylov $\krylov[A, v]{n}$, en notant $H_n$ la matrice carrée égale à $\widetilde{H}_n$ privée de sa dernière ligne, on peut approcher \cite{Saad1992} :
    \[e^Av \approx \norm[2]{v} V_ne^{H_n}e_1\]
    et même plus généralement pour tout $t$ réel :
    \[e^{tA}v \approx \norm[2]{v} V_ne^{tH_n}e_1\ .\]

    \paragraph{}
    Ainsi, dans un solveur utilisant déjà une méthode de Krylov, le code peut être réutilisé pour calculer des produits $e^{tA}v$ et donc utiliser des méthodes exponentielles.
    L'utilisation des méthodes exponentielles se fait de plus en plus connaître, et on trouve des applications à des problèmes de dynamique des fluides réactive avec des méthodes exponentielles explicite \cite{BhattKhaliqWade2018} comme implicite \cite{NieZhangZhao2006}.

    \PS{J'ai d'autres références sur les méthodes exponentielles, mais est-ce que ça vaut le coup de les mettre ?}


  \subsection{Autre méthodes}

    \paragraph{}
    Si ce rapport tente de rassembler les méthodes les plus connues il existe bien entendu d'autres méthodes pour l'intégration temporelle des équations différentielles.
    Certaines de ces méthodes sont listées dans cette section.

    \subsubsection{Intégration parallèle en temps}

      \paragraph{}
      Pour l'intégration parallèle en temps, le paradigme change quelque peu.
      On cherche à intégrer notre équation différentielle sur un segment $\left[0, T\right]$, que l'on subdivise en $N$ segments $\left[t_n, t_{n+1}\right]$, avec $t_{n+1} - t_n = \Delta t$.
      On définit ensuite la fonction $I_{\Delta t}$ associé à notre méthode d'intégration numérique, qui admet comme image de $U_n$ le résultat donné par la méthode lorsqu'on intègre l'équation différentielle sur un temps $\Delta t$ à partir de la condition initiale $U_n$.
      Pour qu'il y ait continuité de la solution entre chaque segment temporel, il faut, en plus de la condition initiale $U_0 = W_0$,
      \[\forall n\in  [\![0, N[\![\ ,\quad I_{\Delta t}\left(U_n\right) = U_{n+1}\ .\]

      \paragraph{}
      On peut alors reformuler le problème autrement, les valeurs aux extrémités des segments $\bar{U} = \transpose{\left(U_0, \dots, U_N\right)}$ forment la solution de :
      \[F\left(\bar{U}\right) = \begin{pmatrix}U_0 - W_0 \\ \vdots \\ U_{n+1} - I_{\Delta t}\left(U_n\right) \\ \vdots\end{pmatrix} = 0\]
      qui peut être résolue à l'aide d'une méthode de Newton, composante par composante, de manière parallèle.

      \paragraph{}
      Cette méthode inspirée du Multiple Shooting \cite{Nievergelt1964} donne la méthode Pararéel (Parareal en anglais) \cite{LionsMadayTurinici2001}.
      La méthode Pararéel et le Multiple-Shooting sont en pratique équivalent, à une approximation près \cite{GanderVandewalle2007}.
      La différence est qu'avec le Parareal on approche la jacobienne de $I_{\Delta t}$ intervenant dans la résolution de la méthode de Newton par une différence finie d'une méthode d'intégration moins précise, et très facile à calculer, utilisant un plus grand pas de temps.

      \paragraph{}
      D'autres algorithmes d'intégration parallèle en temps existent, et ont été utilisés pour résoudre des écoulements turbulents \cite{Lunet2018}.
      On peut par exemple concevoir un algorithme d'intégration en temps utilisant des intégrateurs exponentiels \cite{GanderGuettel2013}.
      Cependant, si de plus en plus de monde semble s'y intéresser, ils se basent sur une approche totalement différente des méthodes classiques, et nécessitent une refonte totale d'un solveur.
      Ce n'est pas une option dans le cadre de ma thèse, l'objectif n'étant pas de créer un solveur de toute pièce mais d'améliorer un déjà existant.


    \subsubsection{Méthodes spectrales en temps}

      \paragraph{}
      Après avoir traité l'intégration spatiale de manière parallèle, on s'est intéressé à l'intégration temporelle, comme décrit dans la section précédente.
      Il en va de même avec l'analyse spectrale : après avoir développé des méthodes d'intégration spatiale se basant sur de l'analyse spectrale, on fait aujourd'hui de même avec l'intégration temporelle.
      Au départ introduit pour des problèmes periodiques et testés sur des applications de dynamique des fluides \cite{GopinathJameson2005, GopinathJameson2006}, cette méthode consiste à prendre la transformée de Fourier de l'équation aux dérivés partielles, de résoudre l'équation obtenue et de prendre la transformée de Fourier inverse de la solution.

      \paragraph{}
      Cette méthode est valable pour les problèmes periodiques dont la période est connue, mais au fil des développements on arrive à des méthodes applicables à des problèmes dont la période n'est pas connue, et même des problème apériodiques \cite{EkiciDjeddiLiEtAl2020}.
      Cependant, tout comme pour les méthodes d'intégration parallèle en temps, l'utilisation de telles méthodes entraînerait une refonte du solveur, ce qui n'est pas souhaité dans le cadre de ma thèse.


    \subsubsection{Modification de l'équation}

      \paragraph{}
      Une solution pour aider à l'obtention d'une solution stationnaire d'une équation aux dérivées partielle peut être de résoudre une autre équation, plus facile à résoudre, menant à la même solution.
      Pour une équation d'advection diffusion, donc par exemple les équations de Navier-Stokes, on propose dans \cite{CouletteFranckHelluyEtAl2019} une méthode de relaxation permettant d'obtenir une méthode plus robuste et parallélisable qu'une méthode implicite classique.
