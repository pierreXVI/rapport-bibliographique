\chapter{Intégration temporelle}

\paragraph{}
Nous avons vu dans l'introduction que lorsqu'on résout un problème de simulation numérique de la dynamique des fluides, on se ramène à résoudre une équation différentielle ordinaire de la forme :
\begin{equation}\label{eq:edo_2}
  Ax = b\ .
\end{equation}
Il existe différents algorithmes permettant de résoudre une telle équation, et la plupart de ces algorithmes se rangent dans deux catégories : les méthodes explicites, et les méthodes implicites.

\section{Analyse des méthodes}

  \paragraph{}
  Il est nécessaire de définir quelques notions pour pouvoir analyser les méthodes qui vont suivre.

  \section{Consistance et ordre}

    \paragraph{}
    Une méthode de résolution d'équations différentielles doit respecter certaines propriétés pour être "correcte".
    Notamment elle se doit d'être consistante.
    Pour définir cette notion, on se place dans le cadre de l'équation (\ref{eq:edo_2}).
    Après un pas, la méthode numérique donne une valeur $W_1$ que l'on souhaite proche de la valeur exacte $W\left(t_0 + \Delta\right)$.
    La méthode est dite consistante si :
    \[\lim_{\Delta t \rightarrow 0} \frac{W_1 - W\left(t_0 + \Delta t\right)}{\Delta t} = 0\ .\]

    \paragraph{}
    De plus, on dira que la méthode est d'ordre $p$ si l'erreur locale est en $\Delta t^{p+1}$ \cite{HairerNorsettWanner1993} :
    \footnote{\GP{J'ai un doute... Ordre p+1 pour moi...}\PS{Voir citation}}
    \[W_1 - W\left(t_0 + \Delta t\right) = O\left(\Delta t^{p+1}\right)\ .\]


  \section{Stabilité}

    \paragraph{}
    Un critère important dans le choix d'une méthode d'intégration est la stabilité.
    En fonction de notre cas d'application, on exigera certains niveaux de stabilité pour éviter une divergence d'origine numérique du calcul.
    Nous cherchons dans cette section à analyser la stabilité d'une méthode d'intégration de l'équation (\ref{eq:edo}).
    Pour simplifier, on considère ici que le pas de temps $\Delta t = t_{n+1} - t_n$ ne dépend pas du numéro de l'itération $n$.
    La taille du problème est notée $N$.

    \paragraph{}
    Pour étudier la stabilité d'une méthode, on utilise en général l'équation différentielle ordinaire avec un second membre linéaire \cite{HairerWanner1996}.
    En effet, si on dispose d'une solution $\tilde{W}$ de (\ref{eq:edo}), on peut linéariser $F$ en $\tilde{W}$ :
    \[\frac{\mathrm{d}W}{\mathrm{d}t} = F\left(\tilde{W}\right) + \frac{\partial F}{\partial W}\left(\tilde{W}\right)\cdot\left(W - \tilde{W}\right)\ .\]
    Si on note ensuite $y = W - \tilde{W}$ et $J = \frac{\partial F}{\partial W}\left(\tilde{W}\right)$, on obtient :
    \begin{equation}\label{eq:stab}
      \frac{\mathrm{d}y}{\mathrm{d}t} = Jy\ .
    \end{equation}
    C'est donc avec cette équation, également appelée équation de Dahlquist, que l'on étudie la stabilité de la méthode.
    Nous l'étudions dans $\mathbb{C}$.


    \subsubsection{Méthodes mono-pas}

      \paragraph{}
      Pour une méthode d'intégration donnée, on note :
      \begin{equation}\label{eq:stab_req}
        y_{n+1} = G\left(\Delta tJ\right)y_n
      \end{equation}
      la relation qui donne l'état suivant en fonction de l'état actuel.
      Pour la plupart des méthodes d'intégration temporelle, et en particulier pour les méthodes de Runge-Kutta présentées ici, on peut se ramener à cette écriture avec $G$ une fonction analytique.
      On peut choisir une base de vecteurs propres de $J$ $v_1, \dots, v_N$ associés aux valeurs propres $\alpha_1, \dots, \alpha_N$.
      Si on décompose l'itéré initial sur cette base, c'est à dire $y_0 = \sum_{i=1}^N\lambda_iv_i$, on voit que puisque $G$ est une fonction analytique :
      \[y_n = \sum_{i=1}^N\lambda_iG\left(\Delta t\alpha_i\right)^nv_i\ .\]

      \paragraph{}
      Il découle de l'équation précédente que si $G\left(\Delta t\alpha_i\right)^n$, alors $y_n$ tend vers 0.
      C'est ainsi qu'on définit le domaine de stabilité d'une méthode d'intégration temporelle :
      \[\left\{\,z\in\mathbb{C}\;\mid\;\left|G\left(z\right)\right| < 1\,\right\}\ .\]
      Si toutes les valeurs propres de $J$ sont à partie réelle négatives, alors la solution de l'équation (\ref{eq:stab}) converge vers 0.
      On comprend alors tout de suite que pour que l'algorithme soit stable, il faut que les valeurs propres de $J$ soient toutes dans le domaine de stabilité de la méthode.

      \paragraph{}
      On remarque que l'argument de $G$ n'est pas $J$ mais $\Delta tJ$.
      Si une valeur propre $\Delta t\alpha_i$ de $\Delta tJ$ n'est pas dans le domaine de stabilité de la méthode, la direction propre associée sera amplifiée et une instabilité d'origine numérique entraînera la divergence du calcul.
      On va donc jouer sur la valeur de $\Delta t$ : en la prenant assez faible, on arrivera à faire rentrer toutes les valeurs propres dans le domaine de stabilité, et donc à garantir la stabilité du calcul.
      Cependant, comme nous le verrons par la suite, cela impose l'utilisation de pas de temps relativement faible, ce qui s'avère contraignant pour nos application.

    \subsubsection{Méthodes multi-pas}

      \paragraph{}
      Certaines méthodes présentées par la suite ne rentrent pas dans le cadre précédent.
      En particulier, les méthodes multi-pas qui seront introduites par la suite ne peuvent pas se réécrire sous la forme (\ref{eq:stab_req}).
      Ces méthodes utilisent les valeurs $y_n, \dots, y_{n-k+1}$ pour trouver $y_{n+1}$.
      Puisque les méthodes présentées par la suite sont linéaires, elles peuvent cependant s'écrire sous la forme :
      \[y_{n+1} = \sum_{i=1}^kg_{k-i}\left(z\right)y_{n+1-i}\ .\]
      On cherche ensuite les $y_i$ sous la forme $y_i\propto\mu^{i}$ ce qui donne :
      \[\mu^k = \sum_{i=1}^kg_{k-i}\left(z\right)\mu^{k-i}\]
      et qui permet d'identifier le polynôme $G_z\left(\mu\right) = \mu^k - \sum_{i=0}^{k-1}g_i\left(z\right)\mu^i$.
      Si les racines de ce polynôme sont toutes de module plus petit que 1, alors la solution converge vers 0.
      On définit ainsi le domaine de stabilité d'une méthode multi-pas \cite{HairerWanner1996} :
      \[\left\{\,z\in\mathbb{C}\;\mid\;\textrm{les racines $\xi$ de $G_z$ vérifient $\left|\xi\right| \leq 1$, les racines doubles vérifient $\left|\xi\right| < 1$}\,\right\}\ .\]


    \paragraph{}
    La propriété clef découlant de cette étude de stabilité est la "A-stabilité" \cite{Dahlquist1963}.
    Une méthode d'intégration temporelle est dite A-stable si son domaine de stabilité inclut le demi-plan des complexe à partie réelle négative.
    Autrement dit, une méthode est A-stable si elle converge bien vers 0 lorsqu'elle le devrait, et ne fait donc pas diverger l'erreur numérique.
    La propriété de A-stabilité nous intéresse beaucoup, car on dit d'une méthode A-stable qu'elle est inconditionnellement stable, alors qu'une méthode qui ne l'est pas est conditionnellement stable.
    Cette appellation signifie qu'une méthode non A-stable nécessite de respecter certains critères, notamment sur le pas de temps, pour qu'elle soit stable, alors qu'une méthode A-stable reste stable quelque soit le pas de temps.


\section{Méthodes explicites}

  \paragraph{}
  Les méthodes explicites cherchent l'état suivant $W_{n+1}$ en fonction des états précédents :
  \[W_{n+1} = F_e\left(W_n, W_{n-1}, \dots, W_0\right)\ .\]
  Ces méthodes sont généralement utilisées dans les simulations numériques instationnaires de la dynamique des fluides.
  L'avantage de ces méthodes est d'une part leur facilité d'implémentation, et d'autre part leur faible coût numérique.
  En effet, puisque l'état recherché est accessible à partir des états précédents, la seule écriture d'une fonction est nécessaire au développement de la méthode, et le coût numérique à chaque itération se résume à l'évaluation de cette fonction.

  \subsection{Méthode d'Euler}

    \paragraph{}
    La méthode d'Euler, ou Euler explicite, est la méthode la plus simple qui soit.
    Elle consiste à intégrer (\ref{eq:edo}) entre $t_n$ et $t_{n+1} = t_n + \Delta t$ en supposant le second membre constant égal à $F\left(W_n\right)$, ou de manière équivalente à remplacer la dérivée dans (\ref{eq:edo}) par le taux d'accroissement : $\frac{\mathrm{d}W}{\mathrm{d}t}\approx\frac{W_{n+1}-W_n}{\Delta t}$.
    Ainsi, cette méthode donne la relation de récurrence explicite :
    \[W_{n+1} = W_n + \Delta t F\left(W_n\right)\ .\]

    \paragraph{}
    Lorsqu'on réalise l'analyse de stabilité de cette méthode, on voit que la fonction $G$ de l'équation (\ref{eq:stab_req}) est $z\mapsto 1 + z$.
    Dans le plan complexe, son domaine de stabilité est donc le disque unité ouvert centré en -1.
    En pratique, ce domaine de stabilité s'avère insatisfaisant car il prohibe l'utilisation de grands pas de temps.


  \subsection{Méthodes de Runge-Kutta}

    \paragraph{}
    Les méthodes de Runge-Kutta forment une classe de méthodes qu'on appellera "multi-étapes", pour établir un contraste avec les méthodes "multi-pas" présentées plus bas.
    Plutôt que de faire un unique pas en avant comme le fait la méthode d'Euler explicite, on va réaliser un ensemble de pas intermédiaires, le pas final étant une combinaison de ces pas intermédiaires.

    \paragraph{}
    Le principe général est le suivant.
    En supposant qu'en $t_n$ on dispose d'une valeur de la solution $W_n$, on introduit les étapes intermédiaires $t_{n,i} = t_n + c_i\Delta t$ pour $1\leq i\leq k$, avec $k$ fixé.
    On peut intégrer entre $t_n$ et $t_{n,i}$ l'équation (\ref{eq:edo}) de manière exacte :
    \[W\left(t_{n,i}\right) = W\left(t_n\right) + \Delta t \int_0^{c_i}F\left(W\left(t_n + s\Delta t\right)\right)\mathrm{d}s\ .\]
    L'intégrale est ensuite approchée par une quadrature sur les points $j<i$ :
    \[\int_0^{c_i}F\left(W\left(t_n + s\Delta t\right)\right)\mathrm{d}s \approx \sum_{j = 1}^{i-1}a_{ij}F\left(W\left(t_{n,j}\right)\right)\ .\]
    Les étapes déjà calculées permettent donc de calculer la quadrature afin d'obtenir l'étape suivante.
    Une fois qu'on a obtenu toutes les étapes intermédiaires, on intègre (\ref{eq:edo}) entre $t_n$ et $t_{n+1}$ :
    \[W_{n+1} = W_n + \Delta t \int_0^1F\left(W\left(t_n + s\Delta t\right)\right)\mathrm{d}s\]
    que l'on approche de nouveau par la quadrature :
    \[\int_0^1F\left(W\left(t_n + s\Delta t\right)\right)\mathrm{d}s \approx \sum_{i = 1}^kb_iF\left(W\left(t_{n,i}\right)\right)\ .\]

    \paragraph{}
    En résumé, en notant les étapes de calcul $W_{n,i} \approx W\left(t_{n,i}\right)$, une méthode de Runge-Kutta réalise une itération de la manière suivante :
    \begin{equation}\label{eq:rk}
      \left\{\begin{aligned}
        W_{n+1} &= W_n + \Delta t\sum_{i = 1}^kb_iF\left(W_{n,i}\right) \\
        \;\textrm{avec}\quad W_{n,i} &= W_n + \Delta t\sum_{j = 1}^{i-1}a_{ij}F\left(W_{n,j}\right)
      \end{aligned}\right.\ .
    \end{equation}

    \paragraph{}
    Une méthode de Runge-Kutta est donc caractérisée par sa taille $k$ et par les valeurs $a_{ij, 1\leq j<i\leq k}$, $b_{i, 1\leq i\leq k}$ et $c_{i, 1\leq i\leq k}$.
    Ces valeurs de quadrature sont souvent représentées dans le tableau de Butcher :
    \[
    \begin{array}{c|c}
      c & A \rule[-1.1ex]{0pt}{0pt} \RKBar \transpose{b}
    \end{array}
    \qquad = \qquad
    \begin{array}{c|ccccc}
      0\\
      c_2    & a_{21} \\
      c_3    & a_{31} & a_{32} \\
      \vdots & \vdots &        & \ddots\\
      c_k    & a_{k1} & a_{k2} & \hdots & a_{k,k-1} \RKBar
      b_1    & b_2    & \hdots & b_{k-1} & b_k
    \end{array}
    \ .\]

    \begin{table}\begin{tabular}{P{.15\textwidth}P{.3\textwidth}P{.4\textwidth}}
      \begin{tabular}{c|c}
        0 \RKBar 1
      \end{tabular} &
      \begin{tabular}{c|cc}
        0 \\ 1/2 & 1/2 \RKBar 0 & 1
      \end{tabular} &
      \begin{tabular}{c|cccc}
        0 \\ 1/2 & 1/2 \\ 1/2 & 0 & 1/2 \\ 1 & 0 & 0 & 1 \RKBar 1/6 & 1/3 & 1/3 & 1/6
      \end{tabular} \\
      RK1 & RK2 & RK4 \\
    \end{tabular}\caption{}\label{tab:rk_butcher}\end{table}

    \paragraph{}
    On rappelle dans la table \ref{tab:rk_butcher} les tableau de Butcher des méthodes les plus connues.
    La méthode RK1 est équivalente à la méthode Euler explicite présentée précédemment.
    Cette méthode RK2 est également appelée méthode du point milieu.
    Cette méthode RK4 est d'ordre 4, et est l'une des plus utilisées pour l'intégration explicite des équations différentielles.

    \paragraph{}
    On peut montrer que l'ordre de la méthode $p$ est tel que $p \leq k$.
    Jusqu'à $k = 4$ on a $k = p$.
    Pour les ordres supérieurs, obtenir une borne entre l'ordre et la taille de la méthode est encore un problème ouvert.
    Pour une méthode de Runge-Kutta d'ordre $p$, la fonction $G$ de l'équation (\ref{eq:stab_req}) est telle que \cite{HairerWanner1996}:
    \[G\left(z\right) = 1 + z + \frac{z^2}{2} + \dots + \frac{z^p}{p!} + O\left(z^{p+1}\right)\ .\]
    Lorsque la méthode est telle que l'ordre $p$ est égal à la taille $k$, le terme en $O\left(z^{p+1}\right)$ est nul.
    On peut donc tracer sur la figure \ref{fig:rk_stab} les domaines de stabilité des méthodes de Runge-Kutta.

    \begin{figure}
      \centering
      \includegraphics[width=.45\textwidth]{images/rk_stab.png}
      \caption{Domaines de stabilité (en couleur) des méthodes de Runge-Kutta pour les quatre premiers ordres.}
      \label{fig:rk_stab}
    \end{figure}

    \paragraph{}
    On constate alors que les méthodes de Runge-Kutta ne sont pas A-stables, et en particulier n'ont pas un large domaine de stabilité.
    En pratique, cela imposera l'utilisation de faibles pas de temps, chose acceptable pour des calculs instationnaires mais insatisfaisante pour nos calculs stationnaires.
    Si le coût de la méthode est moindre, le nombre d'itérations nécessaires pour converger vers un état stationnaire va rendre le coût de calcul global trop important.


  \subsection{Méthodes d'Adams-Bashforth}

    \paragraph{}
    Les méthodes d'Adams-Bashforth sont des méthodes de type "multi-pas" (multi-step), c'est à dire qu'elles utilisent plusieurs états précédents pour déterminer l'état suivant.
    À la différence des méthodes de Runge-Kutta qui utilisent des pas intermédiaires entre $t_n$ et $t_{n+1}$, ces méthodes utilisent les $t_{i\leq n}$ précédents.
    Pour une méthode d'ordre $k$, on utilise donc les états $W_n, \dots, W_{n-k+1}$ pour déterminer $W_{n+1}$.
    On peut en effet vérifier que pour ces méthodes, l'indice $k$ désignant la méthode est égal à l'ordre de la méthode \cite{HairerNorsettWanner1993}.

    \paragraph{}
    L'idée est d'interpoler le second membre de (\ref{eq:edo}) en ces $k$ points calculés précédemment.
    Il existe en effet un unique polynôme $F_k$ tel que pour $0 \leq i < k$ on ait $F_k\left(t_{n-i}\right) = F\left(W_{n-i}\right)$.
    On fait ensuite l'hypothèse $F_k\left(t\right) \approx F\left(W\left(t\right)\right)$.
    On peut intégrer l'équation (\ref{eq:edo}) :
    \[W_{n+1} = W_n + \int_{t_n}^{t_{n+1}}F_k\left(t\right)\mathrm{d}t\ .\]

    \paragraph{}
    Contrairement aux méthodes de Runge-Kutta, puisqu'on réutilise les pas précédents, une unique évaluation du second membre de (\ref{eq:edo}) est nécessaire à chaque itération.
    Le coût de calcul est donc très faible pour de telles méthodes.
    Cependant, les propriétés de stabilité de ces méthodes sont moindres, ce qui explique le fait qu'elles ne sont pas souvent utilisées en pratique dans des codes de calcul pour la dynamique des fluides.

    \begin{figure}
      \centering
      \includegraphics[width=.6\textwidth]{images/ab_stab.png}
      \caption{Domaines de stabilité (en couleur) des méthodes d'Adams-Bashforth pour les quatre premiers ordres.}
      \label{fig:ab_stab}
    \end{figure}

    \paragraph{}
    L'analyse de la stabilité des méthodes d'Adams-Bashforth est plus complexe \cite{HairerNorsettWanner1993, HairerWanner1996}, et le développement des calculs ne sera donc pas détaillé ici.
    On peut toutefois tracer numériquement les domaines de stabilité des premières méthodes sur la figure \ref{fig:ab_stab}.
    On y constate que la stabilité de ces méthode décroît avec l'ordre, et qu'elles ne sont pas adaptées pour les problèmes raides que l'on cherche à résoudre.
    La stabilité de ces méthodes s'avère donc décevante.
    De manière générale, il n'existe pas de méthodes multi-pas explicites qui sont A-stables.


\section{Méthodes implicites}

  \paragraph{}
  Nous avons pu mettre en évidence dans la section précédente le principal défaut des méthodes explicites.
  Si le coût numérique d'une itération de ces méthodes est faible, leur stabilité l'est également.
  Ces mauvaises propriétés de stabilités imposent l'utilisation de faibles pas de temps, ce qui s'avère contraignant lorsqu'on cherche la valeur de l'état convergé, après un temps relativement long.

  \paragraph{}
  \GP{Commentaires pour la suite : Opposé à... bof... et dans les 2 cas, on résoud une équation pour trouver la solution. Je changerais la formulation. Et toi?}
  \PS{Changé.}
  Une solution est d'utiliser les méthodes implicites.
  Si les méthodes explicites expriment l'état suivant à partir de l'état actuel (et éventuellement des précédents), les méthodes implicites donnent cet état suivant comme solution d'une équation.

  \subsection{Méthode d'Euler}

    \paragraph{}
    Pour bien comprendre le fonctionnement d'une méthode implicite, on prend à nouveau la méthode d'Euler, mais cette fois sa version implicite.
    On intègre à nouveau (\ref{eq:edo}) entre $t_n$ et $t_{n+1}$ en supposant le second membre constant, mais cette fois on le prend égal à $F\left(W_{n+1}\right)$.
    On obtient alors l'équation :
    \[W_{n+1} - W_n = \Delta tF\left(W_{n+1}\right)\ .\]
    L'état suivant est donc donné comme étant un zéro de la fonction $z\mapsto z - W_n - \Delta tF\left(z\right)$.

    \paragraph{}
    On peut de nouveau réaliser l'analyse de stabilité de cette méthode.
    Appliqué à l'équation de Dahlquist (\ref{eq:stab}), la méthode d'Euler implicite donne :
    \[W_{n+1} - W_n = \Delta tJ\cdot W_{n+1}\ .\]
    Ainsi, on peut se ramener à la forme de l'équation (\ref{eq:stab_req}) :
    \[W_{n+1} = \left(\operatorname{Id} - \Delta tJ\right)^{-1}W_n\]
    où on peut identifier la fonction de stabilité : $G\left(z\right) = \left(1-z\right)^{-1}$.
    Le domaine de stabilité de la méthode d'Euler implicite est donc le plan complexe privé du cercle unité fermé centré en 1.
    En particulier, le demi-plan complexe des nombres à partie réelle négative est inclus dans le domaine de stabilité, et donc cette méthode est A-stable.


  \subsection{Méthodes de Runge-Kutta}

    \paragraph{}
    Les méthodes de Runge-Kutta ont elles aussi leur équivalent implicite.
    La différence avec la version explicite est qu'à chaque étape, la quadrature de l'intégrale est approchée par l'ensemble des étapes intermédiaires :
    \[\int_0^{c_i}F\left(W\left(t_n + s\Delta t\right)\right)\mathrm{d}s \approx \sum_{j = 1}^ka_{ij}F\left(W\left(t_{n,j}\right)\right)\ .\]

    \paragraph{}
    La méthode de Runge-Kutta implicite réalise l'itération suivante :
    \begin{equation}\label{eq:rk_impl}
      \left\{\begin{aligned}
        W_{n+1} &= W_n + \Delta t\sum_{i = 1}^kb_iF\left(W_{n,i}\right) \\
        \;\textrm{avec}\quad W_{n,i} &= W_n + \Delta t \sum_{j = 1}^ka_{ij}F\left(W_{n,j}\right)
      \end{aligned}\right.\ .
    \end{equation}
    Par rapport à la méthode classique (\ref{eq:rk}), on voit que toutes les étapes de calcul interviennent lorsqu'on veut en calculer une.
    Ainsi, pour chaque étape de la méthode Runge-Kutta implicite, on est amené à résoudre une équation non-linéaire.
    Le surcoût de chaque itération est alors non négligeable.

    \paragraph{}
    Le caractère implicite d'une méthode de Runge-Kutta se voit dans le tableau de Butcher.
    Pour une méthode explicite, la matrice $A = \left(a_{ij}\right)_{1\leq i, j\leq k}$ est triangulaire inférieure.
    Pour la version implicite, elle est quelconque.
    Concrètement, cela veut dire qu'a l'étape $i$, on a besoin de toutes les étapes $j$ telles que $a_{ij}\neq 0$, sans avoir nécessairement $j < i$.
    Ainsi, les étapes doivent se résoudre simultanément en calculant la solution d'un système de $k$ équations non-linéaires.
    On rappelle alors que chaque équation est en fait déjà une équation de dimension $N$ supposée grande.
    On comprend donc pourquoi ces méthodes sont particulièrement coûteuses d'un point de vue numérique.

    \paragraph{}
    Malgré leur coût, les méthodes de Runge-Kutta implicites séduisent la communauté numérique.
    La principale raison est qu'elles permettent d'atteindre un ordre élevé.
    Par exemple, les méthodes basées sur des quadratures de Gauss-Legendre de taille $k$ ont un ordre $2k$ \cite{Iserles2008}.
    On peut donc en théorie atteindre un ordre arbitrairement grand avec ces méthodes.

    \paragraph{}
    Si ces méthodes sont bien plus coûteuses que leur équivalents explicites, elle possèdent de meilleures propriétés de stabilité.
    On applique la méthode à l'équation (\ref{eq:stab}).
    Pour simplifier l'étude, on se place dans le cas scalaire ou de manière équivalente sur une direction propre de $J$ associée à la valeur propre $\alpha$.
    On a alors :
    \[y_{n+1} = y_n + \Delta t\alpha \sum_{i = 1}^k b_iy_{n,i}\]
    et pour $1\leq i\leq k$ :
    \[y_{n,i} = y_n + \Delta t\alpha \sum_{j = 1}^k a_{ij}y_{n,j}\ .\]
    En notant $y = \transpose{\left(y_{n,1}, \dots, y_{n,k}\right)}$ et $e = \transpose{\left(1, \dots, 1\right)}$, on a que :
    \begin{align*}
      y &= y_ne + \Delta t\alpha Ay \\
      \Rightarrow y &= y_n\left(\operatorname{Id} - \Delta t\alpha A\right)^{-1}e\ .
    \end{align*}    Ainsi, pour chaque étape de la méthode Runge-Kutta implicite, on est amené à résoudre une équation non-linéaire.

    Ainsi :
    \[y_{n+1} = \left(1 + \Delta t\alpha\transpose{b}\left(\operatorname{Id} - \Delta t\alpha A\right)^{-1}e\right)y_n\ .\]
    On peut donc clairement identifier la fonction $G$ : $z \mapsto 1 + z\transpose{b}\left(\operatorname{Id} - zA\right)^{-1}e$.

    \paragraph{}
    Il existe donc un grand nombre de méthodes de Runge-Kutta implicites, car tout comme pour les méthodes explicites il existe un grand nombre de quadratures possibles.
    Cependant, certaines méthodes sont plus utilisées que d'autres.

    \subsubsection{DIRK}

      \paragraph{}
      Les méthodes DIRK \cite{Alexander1977}, ou Diagonally Implicit Runge-Kutta methods, consistent à prendre la matrice $A$ du tableau de Butcher comme étant triangulaire inférieure.
      Ainsi, seul les termes $a_{ii}$ sur la diagonale de $A$ donnent le caractère implicite à la méthode.
      À chaque itération, plutôt que de résoudre simultanément $k$ équations implicites de taille $N$ et non-linéaires, on propose ici de les résoudre successivement.
      L'équation (\ref{eq:rk_impl}) donnant l'intermédiaire de calcul devient :
      \begin{equation}\label{eq:rk_dirk}
        W_{n,i} = W_n + \Delta t\sum_{j = 1}^{i-1}a_{ij}F\left(W_{n,j}\right) + \Delta ta_{ii}F\left(W_{n,i}\right)
      \end{equation}

      \paragraph{}
      En général, on résout une équation implicite en la linéarisant pour se ramener à la résolution d'un système matriciel.
      Dans ce cas, la matrice à inverser à l'étape $i$ est : $\operatorname{Id} - \Delta ta_{ii}\frac{\partial F}{\partial W}$.
      On remarque alors que si les $a_{ii}$ sont tous égaux, alors la matrice à inverser est la même pour chaque étape de la méthode.
      En pratique, c'est un gros avantage car on peut alors factoriser la matrice une seule fois pour l'inverser à chaque étape.
      \GP{Référence?}
      \PS{Pas de "référence", c'est un constat immédiat : la matrice est la même pour chaque étape donc on la factorise une seule fois.}
      Cette variante est appelée SDIRK (Singly Diagonally Implicit) \cite{HairerWanner1996}.


    \subsubsection{Méthodes de Rosenbrock}

      \paragraph{}
      Les méthodes de Rosenbrock sont également appelées "linearly implicit Runge-Kutta methods".
      Le principe est le suivant.
      On part d'une méthode DIRK.
      En prenant l'image par $F$ de l'équation (\ref{eq:rk_dirk}), puis en linéarisant autour de $g_i = W_n + \Delta t\sum_{j = 1}^{i-1}a_{ij}F_{n,j}$, on cherche $F_{n,i} \approx F\left(W_{n,i}\right)$ sous la forme :
      \[F_{n,i} = F\left(g_i\right) + \Delta ta_{ii}\frac{\partial F}{\partial W}\left(g_i\right)F_{n,i}\ .\]
      Pour ne pas calculer la jacobienne de $F$ à chaque étape, on prendra $\frac{\partial F}{\partial W}\left(g_i\right) = \frac{\partial F}{\partial W}\left(W_n\right) = F'$.
      En pratique, pour permettre plus de liberté dans le choix de la méthode de Rosenbrock, on remplacera le terme $a_{ii}F'F_{n,i}$ par $F'\sum_{j=1}^i\gamma_jF_{n,j}$.
      \footnote{\GP{Tu ne dis pas les conséquences mathématiques d'un tel changement}\PS{Pas non plus dans le texte de référence, je peux cependant remonter à la source pour trouver ça, mais est-ce que c'est pas un peu m'éloigner du sujet ?}}
      La méthode est donc :
      \begin{equation}\label{eq:rk_rosenbrock}
        \left\{\begin{aligned}
          W_{n+1} &= W_n + \Delta t\sum_{i = 1}^kb_iF_{n,i} \\
          \;\textrm{avec}\quad F_{n,i} &= F\left(W_n + \Delta t\sum_{j = 1}^{i-1}a_{ij}F_{n,j}\right) + \Delta tF'\sum_{j=1}^i\gamma_jF_{n,j}
        \end{aligned}\right.\ .
      \end{equation}

      \paragraph{}
      Une méthode de Rosenbrock est donc identifiée par les coefficients $a_{ij, 1\leq j<i\leq k}$, $b_{i, 1\leq i\leq k}$ et $\gamma_{ij, 1\leq j\leq i\leq k}$.
      Pour inverser toujours la même matrice dans une étape, on veillera à prendre les $\gamma_{ii} = \gamma$ égaux entre eux.
      On inverse donc à chaque étape la matrice $\operatorname{Id} - \Delta t\gamma F'$, et pour cela on ne réalise qu'une unique factorisation par itération de la méthode.


  \subsection{Méthodes BDF}

    \paragraph{}
    Les méthodes BDF (Backward Differentiation Formula) sont des méthodes implicites de type multi-pas.
    Pour une méthode d'indice $k$, on dispose des valeurs de la solution $W_{n-k+1}, \dots, W_n$, calculées en $t_{n-k+1}, \dots, t_n$.
    On forme alors le polynôme $p$ de degré $k$ qui est l'interpolation de Lagrange en les points $\left(\left(t_{n-k+1}, W_{n-k+1}\right), \dots, \left(t_n, W_n\right), \left(t_{n+1}, W_{n+1}\right)\right)$.
    Le principe de la méthode d'évaluer (\ref{eq:edo}) en $t_{n+1}$, en remplaçant $\frac{\mathrm{d}W}{\mathrm{d}t}\left(t_{n+1}\right)$ par $p'\left(t_{n+1}\right)$ et $W\left(t_{n+1}\right)$ par $W_{n+1}$.
    En utilisant la formule de l'interpolation de Lagrange, et le fait que $t_i = t_0 + i\Delta t$, on peut exprimer analytiquement la méthode BDF :
    \[\left(\sum_{i=1}^k\frac{1}{i}\right)W_{n+1} + \sum_{i=1}^k\frac{\left(-1\right)^i}{i}\binom{k}{i}W_{n-i} = \Delta tF\left(W_{n+1}\right)\ .\]

    \paragraph{}
    Le nom de cette méthode vient du fait qu'on peut la reformuler en utilisant la différenciation vers l'arrière (backward differenciation).
    En définissant par récurrence l'opérateur de différenciation $\nabla^0W_i = W_i$ et $\nabla^j+1W_i = \nabla^jW_i - \nabla^jW_{i-1}$, on peut alors réécrire la méthode BDF sous la forme :
    \[\sum_{i=1}^k\frac{1}{i}\nabla^iW_{n+1} = \Delta tF\left(W_{n+1}\right)\ .\]

    \paragraph{}
    L'analyse de stabilité des méthodes BDF est réalisée numériquement.
    Les domaines de stabilité obtenus sont satisfaisants, comme on peut le voir sur la figure \ref{fig:bdf_stab}.
    En particulier, on observe que les méthodes d'indice 1 et 2 sont A-stables.

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{images/bdf_stab.png}
      \caption{Domaines de stabilité (en couleur) des méthodes BDF.}
      \label{fig:bdf_stab}
    \end{figure}

    \paragraph{}
    Pour les méthodes BDF, on montre que l'indice de la méthode est égal à l'ordre.
    Ces méthodes permettent donc d'atteindre un ordre voulu dans entraîner une trop grosse complexité algorithmique.
    En effet, l'équation implicite n'est pas plus difficile à résoudre lorsqu'on monte en ordre.
    Cependant, il n'est pas possible de prendre un ordre aussi élevé qu'on ne le souhaite : les méthodes BDF d'ordre supérieur à 6 sont intrinsèquement instables.
    De manière générale, les méthodes multi-pas ne peuvent pas être A-stable avec un ordre supérieur à 2 \cite{HairerWanner1996}.


\paragraph{}
Cette étude des méthodes d'intégration classiques permet d'orienter le domaine de recherche.
En effet, comme nous le disions au début, nous cherchons à intégrer une équation différentielle ordinaire afin d'obtenir la valeur de l'état après un temps long.
Ainsi, une méthode explicite et ses contraintes sur le pas de temps ne serait pas adaptée.
Si ces méthodes sont en général bien moins coûteuse au niveau informatique, les méthodes implicites restent le meilleur choix pour nos problèmes raides.
Il est souvent plus rentable d'effectuer une unique itération, coûteuse mais sur un grand pas de temps, d'une méthode implicite, que d'effectuer tout un ensemble d'itérations, peu coûteuses mais sur un petit pas de temps, d'une méthode explicite.


\section{Étapes du calcul}

	\paragraph{}
	Nous cherchons à résoudre l'équation (\ref{eq:edo}) en utilisant une méthode d'intégration numérique de type implicite.
  Pour ces méthodes, on recherche l'état, ou de manière équivalente l'incrément de l'état $\Delta W = W_{n+1} - W_n$, en l'exprimant comme le zéro d'une fonction.
  Par exemple, pour la méthode d'Euler appliquée à (\ref{eq:edo}), on cherche à chaque itération le zéro de $\delta W \mapsto \delta W - \Delta t F\left(W_n + \delta W\right)$.
	En notant $G$ cette fonction, on obtient l'équation non-linéaire :
	\begin{equation}\label{eq:non_linear}
		G\left(\delta W\right) = 0
	\end{equation}
	Nous allons donc devoir résoudre une équation non-linéaire à chaque itération.
  On remarque en fait que pour toutes les méthodes d'intégration implicite on a besoin de résoudre une ou plusieurs équations non-linéaires de la forme de l'équation (\ref{eq:non_linear}).
  C'est pour cela que dans le développement d'un code de CFD, il est important de pouvoir résoudre de telles équations.

	\paragraph{}
	Pour résoudre l'équation (\ref{eq:non_linear}), on utilise par exemple la méthode de Newton.
	On part d'une valeur initiale de l'incrément $\delta W_0 = 0$, mais on peut envisager de prendre d'autres valeurs, comme $\Delta t F\left(W_n\right)$ qui est l'incrément donné par la méthode d'Euler explicite.
	On va ensuite itérer pour trouver le zéro de $G$.
	La linéarisation à l'ordre 1 de l'équation (\ref{eq:non_linear}) s'écrit :
	\[\left(\operatorname{Id} - \Delta t\frac{\partial F}{\partial W}\left(W_n\right)\right)\delta W -\Delta t F\left(W_n\right) = 0\ .\]
	On exprime alors l'incrément $\delta W$ comme étant la solution d'un système linéaire :
	\begin{equation}\label{eq:linear}
		A\delta W = b
	\end{equation}
	avec la matrice $A = \operatorname{Id} - \Delta t\frac{\partial F}{\partial W}\left(W_n\right)$ et le second membre $b = \Delta t f\left(W_n\right)$.
	La méthode de Newton consiste ensuite à répéter le calcul précédent, pour faire converger la valeur de $\delta W$.

	\paragraph{}
	Pour toutes les méthodes d'intégration temporelles implicites, on retrouve la même idée : pour chaque itération de la méthode, on résout une ou plusieurs équation non-linéaire (\ref{eq:non_linear}), en résolvant un ou plusieurs systèmes linéaires (\ref{eq:linear}).
	Les performances d'un code de calcul sont donc très étroitement liées à sa capacité à résoudre efficacement ces systèmes linéaires.
